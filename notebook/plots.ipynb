{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"glassermanli1.txt\", \"glassermanli2.txt\", \"glassermanli3.txt\", \"glassermanli4.txt\"]\n",
    "converters={'mu': list, 'glassermanli': float, 'l': float}\n",
    "dfs = map(lambda x: pd.read_csv(\"../\" + x, sep='\\t', converters=converters), paths)\n",
    "glassermanlidf = pd.concat(dfs)\n",
    "glassermanlidf = glassermanlidf.rename(columns={\"glassermanli\": \"mc\"})\n",
    "for k in ['mc']:\n",
    "    print(scipy.stats.describe(glassermanlidf[k]))\n",
    "print(len(glassermanlidf))\n",
    "glassermanlidf['algo'] = 'glassermanli_useoptimization'\n",
    "glassermanlidf.head()\n",
    "glassermanlidf.mc.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"gl1.txt\", \"gl2.txt\", \"gl3.txt\"]\n",
    "dfs = map(lambda x: pd.read_csv(\"../\" + x), paths)\n",
    "gldf = pd.concat(dfs)\n",
    "for k in ['mc']:\n",
    "    print(scipy.stats.describe(gldf[k]))\n",
    "print(len(gldf))\n",
    "gldf['algo'] = 'glassermanli'\n",
    "gldf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../bernoulli2000.txt\"\n",
    "bdf = pd.read_csv(path)\n",
    "for k in ['mc']:\n",
    "    print(scipy.stats.describe(bdf[k]))\n",
    "print(len(bdf))\n",
    "bdf['algo'] = 'bernoulli'\n",
    "bdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_slow_path = os.path.join(\"..\", \"..\", \"dcs_summer\", \"fullcredit\", \"RecreatingBinaryCreditProblem\", \"GlassermanIS_slow\")\n",
    "logpaths = [\n",
    "    \"compare_methods_S5_l0.01.txt\",\n",
    "    \"compare_methods_S5_l0.03.txt\",\n",
    "    \"compare_methods_S5_l0.04.txt\",\n",
    "    \"compare_methods_S5_l0.05.txt\",\n",
    "    \"compare_methods_S5_l0.06.txt\",\n",
    "    \"compare_methods_S5_l0.10.txt\",\n",
    "    \"compare_methods_S5_l0.15.txt\",\n",
    "    \"compare_methods_S5_l0.20.txt\",\n",
    "    \"compare_methods_S5_l0.25.txt\",\n",
    "    \"compare_methods_S5_l0.30.txt\",\n",
    "    \"compare_methods_S5_l0.80.txt\",\n",
    "]\n",
    "logpaths = map(lambda x: os.path.join(gl_slow_path, x), logpaths)\n",
    "dfs = map(lambda path: pd.read_csv(path, skiprows=[0]), logpaths)\n",
    "slowdf = pd.concat(dfs)\n",
    "slowdf['log_mc'] =  map(lambda x: log(x) if x != 0 else np.nan, slowdf['mean'])\n",
    "slowdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "longrun = defaultdict(list)\n",
    "for (algo, tail), group in slowdf.groupby(['algo', 'tail']):\n",
    "    longrun[algo].append((tail, np.mean(group['log_mc'][10:])))\n",
    "for k, v in longrun.iteritems():\n",
    "    longrun[k] = zip(*v)\n",
    "longrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat((bdf, gldf, glassermanlidf))\n",
    "print(len(df))\n",
    "df['log_mc'] = map(lambda x: log(x) if x != 0 else np.nan, df['mc'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logscale = True\n",
    "mc_key = 'mc' if not logscale else 'log_mc'\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,12)\n",
    "for algo, group in df.groupby(['algo']):\n",
    "    if algo == 'bernoulli':\n",
    "        label = \"{} (NZ,NE)=(2000, 2000) 30nrun/l\".format(algo)\n",
    "        plt.plot(group['l'], group[mc_key], 'o', label=label, alpha=0.8)\n",
    "    elif algo == 'glassermanli':\n",
    "        for mu, group in group.groupby(['mu']):\n",
    "            if mu == 0:\n",
    "                label = \"{} mu:{} (NZ,NE)=(1000, 1000) 10nrun/l\".format(algo, mu)\n",
    "                plt.plot(group['l'], group[mc_key], 'o', label=label, alpha=0.8)\n",
    "    else:\n",
    "        label = \"outer/inner optimization (NZ,NE)=(1000,1000) 10run/l\"\n",
    "        plt.plot(group['l'], group[mc_key], 'o', label=label, alpha=0.8)\n",
    "            \n",
    "    \n",
    "# for algo, (xs, ys) in longrun.iteritems():\n",
    "#     label = \"{} (longrun)\".format(algo)\n",
    "#     plt.plot(xs, ys, '-', label=label, lw=5)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim((-0.01,0.62))\n",
    "plt.ylim((-22.5, 2))\n",
    "plt.xlabel(\"tail l\")\n",
    "plt.ylabel(\"Log MC estimate\")\n",
    "plt.title(\"Log MC estimates for ~1200runs of bernoulli_mc and ~200runs of glassermanli_mc vs. tail l\")\n",
    "plt.savefig(\"plt.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
